Date: [[2023-05-18]]

Status: #notes

Tags: #ispr[[Machine Learning]], [[A.I. Master Degree @Unipi]].

# Posterior of the course


$$
P(ISPR|D) = P(ISPR)P(D|ISPR)
$$


Latent representation and what are the challenges when we have to do with them. Why are nice and so on.

State of the art stuff like attention in flavours, adversarial learning is useful for when we want to regularize some model to make it behave with respect to a certain distribution...
Reinforcement learning comes useful in general like bootstrapping, low variance and high bias at the start.

Can you derive EM for GMM like answers! Big tech ask stuff about statistich to test knowledge. SOTA models changes a lot, theory not.


### Take home lesson

```ad-summary


```


---
# References

