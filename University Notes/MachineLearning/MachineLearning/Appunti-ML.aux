\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Neural Networks}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Regularization}{5}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Stopping Criteria}{6}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}VC-Dimension}{6}{section.1.3}\protected@file@percent }
\newlabel{VC-dimension-space}{{1.3}{6}{VC-Dimension}{theorem.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Statistical Learning Theory}{7}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Intro to SVM}{7}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Linear SVM}{7}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Quadratic Optimization Problem}{9}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Why is this elegant?}{12}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Mapping to high-dimensional space}{13}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kernel trick}{13}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Kernel Matrix}{14}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Algebra of Kernels}{15}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}SVM and Neuron architecture}{15}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Well known Kernels}{16}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}SVM for non-linear regression}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What you should know?}{17}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Practical aspects of SVM}{18}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Historical SVM application}{19}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Associate distance to kernels}{20}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Bias-Variance}{21}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Bias-Variance analysis}{22}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Recall of statistics}{22}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Decomposition}{22}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Bias-Variance and Regularization}{24}{subsection.1.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Bias-Variance trade-off}}{24}{figure.1.1}\protected@file@percent }
\newlabel{fig:bias_variance}{{1.1}{24}{Bias-Variance trade-off}{figure.1.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning}{25}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Inductive bias arguments}{25}{subsection.2.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Curse of dimensionality and Bias}{25}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.2}Representation learning}{26}{subsection.2.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Autoencoders}{27}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{AlexNet}{29}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.3}Changing design of Networks for different applications}{29}{subsection.2.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Distributed Representations}{29}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Internal representation and input}{30}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Disentangling concepts}{31}{subsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Word Embedding}}{31}{figure.2.1}\protected@file@percent }
\newlabel{fig:word_embedding}{{2.1}{31}{Word Embedding}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Beyond Neural Networks}{32}{subsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Smoothness trough network}{33}{subsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Non overfitting puzzle}{33}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Lottery ticket hypotesis}{34}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Deep Learning technicalities}{34}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Techniques}{35}{section*.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Gradient clipping}}{36}{figure.2.2}\protected@file@percent }
\newlabel{fig:gradient_clipping}{{2.2}{36}{Gradient clipping}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}ReLu in DL}{37}{subsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Batch Normalization}{37}{subsection.2.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Unsupervised Learning and Clustering}{38}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Clustering}{38}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Vector Quantization}{38}{section*.17}\protected@file@percent }
\newlabel{discrete_vq}{{2.6}{39}{Vector Quantization}{equation.2.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Online K-means}{40}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{K-means batch algorithm}{40}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax}{41}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Recurrent Neural Networks}{41}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Why Sequantial Data are good?}{42}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces RNN diagram}}{42}{figure.2.3}\protected@file@percent }
\newlabel{fig:rnn_diagram}{{2.3}{42}{RNN diagram}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Memory in RNN}{43}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces IDNN diagram}}{43}{figure.2.4}\protected@file@percent }
\newlabel{fig:idnn_diagram}{{2.4}{43}{IDNN diagram}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Recurrent Units}{43}{section*.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Recurrent Unit diagram}}{43}{figure.2.5}\protected@file@percent }
\newlabel{fig:recurrent_unit_diagram}{{2.5}{43}{Recurrent Unit diagram}{figure.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Turing Machine and RNN}{44}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Proving RNN and TM are equivalent}{44}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}More properties}{45}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Unfolding}{45}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.7}Advanced models}{46}{subsection.2.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.8}Echo state networks}{47}{subsection.2.3.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Echo state network}}{47}{figure.2.6}\protected@file@percent }
\newlabel{fig:echo_state_network}{{2.6}{47}{Echo state network}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.9}Toward structured domains}{48}{subsection.2.3.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Tree RNN}}{48}{figure.2.7}\protected@file@percent }
\newlabel{fig:tree_rnn}{{2.7}{48}{Tree RNN}{figure.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Structured Domains}{48}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Graph RNN}}{48}{figure.2.8}\protected@file@percent }
\newlabel{fig:graph_rnn}{{2.8}{48}{Graph RNN}{figure.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Molecules representation}}{49}{figure.2.9}\protected@file@percent }
\newlabel{fig:molecules}{{2.9}{49}{Molecules representation}{figure.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Recurrent approach from sequence to trees}{50}{subsection.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Tree and sequences}}{50}{figure.2.10}\protected@file@percent }
\newlabel{fig:tree_rnn}{{2.10}{50}{Tree and sequences}{figure.2.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Recursive Cascade Correlation}{50}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Reservoir computing and Trees}{50}{subsection.2.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Tree ESN}}{51}{figure.2.11}\protected@file@percent }
\newlabel{fig:tree_esn}{{2.11}{51}{Tree ESN}{figure.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Self-organizing maps}{51}{subsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}Hidden Tree Markov Models}{51}{subsection.2.4.5}\protected@file@percent }
\gdef \@abspage@last{51}
