Date: [[2023-03-07]]

Status: #notes

Tags: [[Human Language Technologies]], [[A.I. Master Degree @Unipi]]

# Applications and issues

### Generated by ChatGPT issue

Is really difficult to distinguish humans from GPT, there is an AI text classifier by OpenAI from very unlikely to unclear to likely, but is very poor accuracy wise.
Another attempt is GPTZero by a student in princepton

### Classify articles in medical literature

Medline is a collection of scientific articles, classified in some categories to make easier to find the one that intrest the physicians. More application like : language indentification, sentiment analysis, opinion analysis, labeling domain specific things.

# Definition of Text Classifier

Formally

$$
\begin{align}
f: D \to C \\\\
with \; d \in D \; input \;doc \\\\
set \; of \; classes: \; C \\\\
Training \; set: N \\\\
\gamma: D \to C \; classifier

\end{align}
$$

So what we're trying to do is understand to what class a certain document is part of. For example we can distinguish for spam detection different emails that are our documents.

## Classification methods

For *supervised* classification we have 

- Naive Bayes
- Logistic Regression
- SVM
- K-nearest
- Deep Neural Nets

Recall conditional probability seen in [[Lesson 5 - Generative and Graphical Models#Joint and Conditional Probabilities]]

### Multinomial Naive Bayes classifiers

We represente an instance $D$ as vector of features (words).

$$
D = <x_1,x_2,...,x_n>
$$

The **Task** is to classify a new instance $D$ into one of classes $c_{j}\in C$

$$c_{MAP} = argmax_{c_{j} \in C} P(c_{j}| x_1,...,x_n)$$

Suppose we have 300 in category A and 200 in category B, to assign what cathegory is we can estimate very simply with frequency in classes

By ***conditional independence assumption*** in [[Lesson 5 - Generative and Graphical Models#Independance and Conditional Independance]] we can multiply them. So basically given a class all the $x_i$ are independent.

Features are just the *words* in document

#### Bag of words representation

BoW is a simplification to how we treat our data, we have a word count of the most used word in the text that have the higher frequency, and then the least used ones. We want to represent a *document* in this form, that is the simplest one!


### Naive Bayes learning (Supervised)

Build a vocabulary from a corpus, calculate the matrix that is big at how many words you have and evaluate the probability for each word. So $P(c_j)$ and $P(w_{k}| c_{j})$

For each classes then do this 
![[Pasted image 20230307103529.png]]

For a new document?

Go in log space to simply add probabilities instead of multiplying them, because multiplying a lot of small probabilities yields to a very small number.

### Training NB
![[Pasted image 20230307104151.png]]

### Prediction NB

![[Pasted image 20230307104231.png]]

#### Naive bayes: weakenesses 

- Naive Bayes assumption has two consequences
	- Linear ordering of words is ignored (bag of words)
	- The words are independent of each other given the class.
- Naive bayes assumption is inappropriate if there are strong conditional dependances

### Measure performances
![[Pasted image 20230307104711.png]]

When measuring performances we need to choose a threshold between **Precision**, how many good message we keep against all messages and **Recall** the % of good messages keepen, since good messages are very important we don't want to loose them and ideally we want a 100% recall, but to get higher recall we need to get worse accuracy.
If threshold is high we keep only good stuff, but lose a lot of it as well, at low we keep good and bad stuff, for spam filtering and legal search we prefer 100% recall

Another measure is the ***Weighted Armonic Mean***:
![[Pasted image 20230307105210.png]]
where precision and recall are calculated this way
![[Pasted image 20230307105251.png]]


# Classification

Linearly separable data discussion...

## Decision trees

This model for classification has some rule that applies, the weakness here is that the order of the rules that are being applied does matter, if data are not linearly separable we can't simply use a linear model to discrimite the classes, we need to find non linear boundaries
![[Pasted image 20230308143120.png]]

For example Naive Bayes or Max Entropy are linear models

## Perceptron model

A very simple model, basically a linear hyperplane separating data, for binary classification. Can be seen as well as a single layer neural network
![[Pasted image 20230308143551.png]]

This model recalling the [[Machine Learning]] course, is the first model proposed by **McCulloch-Pits***
![[Pasted image 20230308144327.png]]

Then we had the **Rosemblatt Perceptron** inspired by neuroscience studies and ideas
![[Pasted image 20230308144550.png]]

And then we can have multiple layer of Perceptrons that are now the Neural Networks
And with layers we can have more complex boundaries.
![[Pasted image 20230308144727.png]]

The ***universal theorem of approximation*** says that with two layer perceptron we can describe all the linear continuous function

>[!info]
> 






---
# References

