[[Machine Learning]],[[Statistics]], [[Theoretical Machine Learning]], [[Probability]]

Book suggestion and notes...


# Maths notation and preliminaries on SLT

#### Simple Linear Regression example

Assume two linear related quantities, eg $ of texas crude oil and $ of diesel, givien $\mathcal{D}=\{x_i,y_i\}_{i=1}^n$ dataset

We want to predict values of new data, so searching the best fit for a line with a slope $\beta^*$ and bias. 

So the prediction would be
$$
y_{new} \approx \beta^* x_{new}
$$

### Generalization of the task: Supervised Learning Problem

What can i say about this prediction, how much we're doing wrong?

Given a training set $\mathcal{D}_n$ containing pair $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$ we can have arbitrary dimension in the input space(aka : *features*/covariates/*predictors*)  and output space, we will call (*labels or outcomes*)

### Main assumption IIDness

Datapoint are generated by some distribution $P$ and our points are sampled from the same distribution

$$
(x_i,y_i) \approx^{iid} P(dx,dy) \in \mathcal{M}_+^1(X,Y)
$$
This distribution is not known we can use some methods to approximate it we will use notation $\mathcal{D}_n, (P)$

We can *disintegrate* in measure theory means to condition on our distribution in this way $P(dx,dy) = P_{y|x}(x,dy)P_x(dx)$

```ad-info
Note: If the output space is continuous we spoke about regression so $Y \in R^d$, instead we do classification if it's discrete: $Y \in Z$
```

### Example of $P$ in linear regression

$P_x$ is unknown again

Assuming that the conditional distribution is a normal $\mathcal{N}(\beta^{*}x,1) \iff y_i = \beta^{*}x_i+\epsilon_i$ where $\epsilon_i$ is irreducible noise from an *isotropic* normal

### Example of $P$ in classification

Basically a Bernoulli distribution: $P_{y|x}(x,y) = \mathcal{B}(x)=(1-\pi(x)^\frac{1-y}{2})\pi(x)^\frac{1+y}{2}$ for a given $\pi(x)$

## Bayesian Decision Theory

We would like to find some $f :\mathcal{X} \to {\mathcal{Y}}$ such that for a new input,output pair: 

$$f(x_{new}) \approx y_{new}$$

We need three things: how to perform, so the function, how to decide how well we're approximating and performing the prediction.


We need a measure of performance namely: *loss function*, telling how good is our approximation: we will call this $$l: \mathcal{Y} \times\mathcal{Y} \to R$$ a function taking prediction and ground truth and returning a distance measure or similarity mesure between the two.

This approximation introduce an *Expected Risk* or population risk (generalization error expected): we will use `population risk`

Given a fixed $f$
$$
\mathcal{R}(f) = \int_{X \times Y} e(y_{new, f({x_{new})}P(d_{x_{new}}, d_{y_{new}})})
$$
a proxy for the population risk is the *Empirical Risk* also called training error, that is the error we know, that is usually lower bounded

$$
\hat{R}_n(f) = \frac{1}{n}\sum_i^ne(y_i, f(x_i))
$$

So given a problem we need to use a certain loss that adapts to the problem
### Example of regression loss

Quadratic loss aka MSE loss

$$
R(f) = \int_{X \times Y} MSE(f,x,y)P(dx,dy)
$$
maybe rewerite this later...

for linear regression: $f_\beta: \beta \to \beta . x$

$$
R(\beta) = \int(\beta x-y)^2\frac{1}{\sqrt{2\pi}}e^-\frac{(y-\beta x)^2}{2}
P_x(dx)dy$$

### Example in classification (0,1)

We use *indicator variables*, $e(y,y') = \mathbf{1}(y\neq y')$ = $\mathbf{1}(yy'<0)$

$$
R(f) = \int \mathbf{1}(f(x)\neq y)P(dx,dy) =P(f(x)\neq y)
$$

Another example could be the log loss and visualize connection between loss and entropy

### Example of logistic regression

Take and $f$ that a point goes from 0,1 interval and compute relative entropy define by $f$ with respect to the true probability distribution: this is like when reducing the KL-Divergence


## Model or Hypotesis class

In ML call *model* and statistics use hypotesis class

$$\mathcal{F} \subset {f: X \to Y}
$$
And we want to find the optimal approximator

Computer work in finite space, working in this space is too big (infinitely dimensional).

Usually we work with parametric models

$$
\mathcal{F_\theta} = {f_\theta: X \to Y}
$$

If $\theta$ is a finite space we're reducing our possible search space in a smaller dimension, neural nets is just finding the right parameters $\theta$

$$\Theta \in R^d, \mathcal{F} = {x \to \beta x, \beta \in \Theta}$$

#### Logistic regression

We have for logistic the same, but now our $x$ can be a non-linear function such as sigmoid or tanh...

Where $x = (1+e^{(-\beta x -1)})$

$$\Theta \in R^d, \mathcal{F} = {x \to \beta x, \beta \in \Theta}$$

## NN model...

## Optimization algorithm

Is a map that take our data and gives us a function in the space $\mathcal{F}$

$$
\mathcal{A}: (X \times Y)^n \to \mathcal{F},     \mathcal{D_n} \to \mathcal{A(\mathcal(D_n))}
$$

What we're doing is **Empirical Risk Minimization** this is what is minimize by our algorithm so 

We want $min(\hat R)$ given $\mathcal{F}_\Theta$

Provided that distribution is not degenerate we will have a single $\beta$ minimizing our empirical risk

### Eg linear regression


$\mathcal{\hat R}(\beta) = \frac{1}{n} \sum_i^n(\beta x_i - y_i)^2$ 

We're minimizing from a subset of the distribution 

Think to $\beta x_i$ as a matrix multiplication in $n$ dimensional space the input space $\frac{1}{n} ||\bar y - X\bar\beta||$

$X$ is our matrix of $x_s$ and $y$ is a vector of outputs

Let be $D_\beta$ as derivative with respect to beta: $D_\beta \mathcal{\hat R}(\beta) = X^T(X\beta-y)= 0$

$$
\mathcal{A}(\mathcal{D}_n) = (X^TX)^-1(X^Ty) = \beta(\mathcal{D_n})
$$

There is a geometrical view for the pseudoinverse...
## Summary 

For a given $\mathcal{P}$ problem, $p$ $\mathcal{F}_\theta$, $\mathcal{A}$, we **study**

$\Delta(\mathcal{D_n}) = \mathcal{R}(\mathcal{A}(\mathcal{D_n}) - inf_{f:X\to Y} \mathcal{R(f)}$

Over all possible $f$ functions, this depends on the $n$ of our database, more data we have better we're approximating

For instance we would ask if the expectation of the quantity goes to zero 

This if L1 convergence, another can be convergence in probability

We have a lot of different choose to do and each of them influence the others, also we can have a lot of part moving inside the $\theta$

## Risk decomposition

We would like to decompose the risk in this way.

Often study $\Delta(\mathcal{D_n})$ as it it is hard, we decompose it as a sum of parts

With $\hat\theta$ is an estimator of our optimal theta

What we do is reduce to how bad is the choosen $f$
$$
\mathcal{R(f_\hat\theta)} - \inf_{f:X\to Y}\mathcal{R(f)}=\mathcal{R(f_\hat\theta)} - \inf_{f\in\mathcal{F_\theta}}\mathcal{R(f)} +
\inf_{f \in \mathcal{F_\theta}}\mathcal{R(f)} \inf_{f:X\to Y}\mathcal{R(f)}
$$

The first part is **Estimation error** and the second **Approximation error**

We could further the Estimation error

In empirical model and current model this is **Generalization error** same model on actual risk, how model generalize to unseen data. This depends on the dataset, look that we called the entire risk generalization error so it's kinda difficult. So this is statistics part of the model, given a distribution we compare to the exected value of the same quantity.
$$
\mathcal{R(f_\hat\theta)} - \inf_{f\in\mathcal{F_\theta}}\mathcal{R(f)} = \mathcal{R(f_\hat\theta)- \mathcal{\hat R(f_\hat\theta)}+ \text{Optimization error}}
$$
**Optimization error** depends on $\mathcal{F}$ and it's related to analysis asking: What is the performance on training set given the function we chosen and we're comparing. And this is hard becuse $\mathcal{A(\mathcal{F})}$ is dependant and it's related to optimization theory.

1. So we start by analysis part: how expressive is our neural network (universal approximation theorem)
2. Then we have a part about optimization, how to choose the optimal model... (stochastic gradient descent): stocastic differential equations.
3. Statistics part, no answers to this question. If we talk about infinitely big neural nets as the model becomes and we will study that object: **[[Neural Tangent Kernels]]** (how we initialize network) and in the final part there is the **[[Mean Field Regime]]** seeing in the limit behaviour of networks
