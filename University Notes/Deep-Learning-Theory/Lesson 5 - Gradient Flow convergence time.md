
Last time we discussed on convergence and how train neural networks, we also described gradient descent in more mathematical terms.


How fast with GF and how many amount of steps we need to converge with GF? 

```ad-question
When does GF converges and how can we bound $\hat R(f_\theta) - inf_{f\in\mathcal{F}} \hat R(f)$
```

## Example (Linear regression)

Do we get optimality in some way?

Let's say $n$ data points i bigger than model dimension $d$.

![[Pasted image 20240116155556.png]]


