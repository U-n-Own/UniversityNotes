@InProceedings{pmlr-v238-ceni24a,
  title = 	 {Random Oscillators Network for Time Series Processing},
  author =       {Ceni, Andrea and Cossu, Andrea and W St\"{o}lzle, Maximilian and Liu, Jingyue and Della Santina, Cosimo and Bacciu, Davide and Gallicchio, Claudio},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4807--4815},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/ceni24a/ceni24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/ceni24a.html},
  abstract = 	 {We introduce the Random Oscillators Network (RON), a physically-inspired recurrent model derived from a network of heterogeneous oscillators. Unlike traditional recurrent neural networks, RON keeps the connections between oscillators untrained by leveraging on smart random initialisations, leading to exceptional computational efficiency. A rigorous theoretical analysis finds the necessary and sufficient conditions for the stability of RON, highlighting the natural tendency of RON to lie at the edge of stability, a regime of configurations offering particularly powerful and expressive models. Through an extensive empirical evaluation on several benchmarks, we show four main advantages of RON. 1) RON shows excellent long-term memory and sequence classification ability, outperforming other randomised approaches. 2) RON outperforms fully-trained recurrent models and state-of-the-art randomised models in chaotic time series forecasting. 3) RON provides expressive internal representations even in a small parametrisation regime making it amenable to be deployed on low-powered devices and at the edge. 4) RON is up to two orders of magnitude faster than fully-trained models.}
}

@article{article,
author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Y.},
year = {2013},
month = {12},
pages = {},
title = {How to Construct Deep Recurrent Neural Networks}
}

@article{GALLICCHIO201787,
title = {Deep reservoir computing: A critical experimental analysis},
journal = {Neurocomputing},
volume = {268},
pages = {87-99},
year = {2017},
note = {Advances in artificial neural networks, machine learning and computational intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.12.089},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217307567},
author = {Claudio Gallicchio and Alessio Micheli and Luca Pedrelli},
keywords = {Reservoir computing, Echo State Networks, Deep Learning, Deep neural networks, Recurrent neural networks, Multiple time-scale dynamics},
abstract = {In this paper, we propose an empirical analysis of deep recurrent neural network (RNN) architectures with stacked layers. The main aim is to address some fundamental open research issues on the significance of creating deep layered architectures in RNN and to characterize the inherent hierarchical representation of time in such models, especially for efficient implementations. In particular, the analysis aims at the study and proposal of approaches to develop and enhance hierarchical dynamics in deep architectures within the efficient Reservoir Computing (RC) framework for RNN modeling. The effect of a deep layered organization of RC models is investigated in terms of both occurrence of multiple time-scale and increasing of richness of the dynamics. It turns out that a deep layering of recurrent models allows an effective diversification of temporal representations in the layers of the hierarchy, by amplifying the effects of the factors influencing the time-scales and the richness of the dynamics, measured as the entropy of recurrent units activations. The advantages of the proposed approach are also highlighted by measuring the increment of the short-term memory capacity of the RC models.}
}

@article{memorycapacity,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Short Term Memory in Echo State Networks}
}